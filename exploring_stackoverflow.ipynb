{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1cc55a2",
   "metadata": {},
   "source": [
    "## Stackoverflow survey (2011-2021) exploration ##\n",
    "\n",
    "### Business / real-world questions: ###\n",
    "\n",
    "**1.** Has the background of people approaching the programming world changed over the recent years? And therefore, what kind of people represent a suitable market for IT companies?\n",
    "\n",
    "**2.** Has the programming language of reference changed over the years? Therefore, which programming language should one acquire to be stronger on the market?\n",
    "\n",
    "**3.** Once we know the programming language(s) of reference now, do they also predict the highest remuneration for who works using them? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16e56e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import zipfile\n",
    "import os\n",
    "from collections import defaultdict\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "714d9cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set the main dir and move where we have our zip files.\n",
    "\n",
    "years = list(range(2011,2022)) # 2011-2021\n",
    "main_dir = os.getcwd() # set the current directory as main directory\n",
    "os.chdir(main_dir + '/zip_datasets') # move into the directory where the zip datasets are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50ea31d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the datasets from the Stackoverflow survey and unzip them in specific subdirectories, according to year\n",
    "# which are created iteratively\n",
    "\n",
    "for y in years: \n",
    "    target_dir = main_dir + '/unzip_datasets/' + str(y) # subdirectory \n",
    "    os.makedirs(target_dir) # create the subdirectory\n",
    "    with zipfile.ZipFile('stack-overflow-developer-survey-' + str(y) + '.zip') as zf:\n",
    "        zf.extractall(target_dir) # extract into that directory\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24887d64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now we read all the the CSV files and we save them in different datasets according to the different years\n",
    "# Since this process can take a bit, to reduce time I ask the script to skip corrupted rows and to not use too much memory\n",
    "# it will take about 5 mins. so grab a coffe in the meanwhile maybe :)\n",
    "\n",
    "import chardet # we need this library to detect the encoding of older CSV files\n",
    "\n",
    "d = {} # create a dictionary in which we store every year dataframe\n",
    "for y in years:\n",
    "    if y == 2016: # in 2016 we have an ulterior subdirectory which we need to move into to read the CSV\n",
    "        os.chdir(main_dir + '/unzip_datasets/' + str(y) + '/2016 Stack Overflow Survey Results')\n",
    "    else:\n",
    "        os.chdir(main_dir + '/unzip_datasets/' + str(y))\n",
    "    if y in range(2015,2018): # we need to adjust the index for years 2015,2016,2017 to read the CSV file and not something else\n",
    "        filen = os.listdir()[2]\n",
    "    else:\n",
    "        filen = os.listdir()[0]\n",
    "    rawdata = open(filen, \"rb\").read() # open the file as binary so that we can detect encoding type\n",
    "    enc = chardet.detect(rawdata)['encoding'] # read the encoding type\n",
    "    d['df_' + str(y)]  = pd.read_csv(filen,encoding = enc, error_bad_lines = False, low_memory = True)\n",
    "    print(filen + ' succesfully imported')\n",
    "print('all dataframes imported')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977e9f84",
   "metadata": {},
   "source": [
    "As we can see, not all datasets are in good shape or are correctly uploaded. For later statistics we only focus on gender and country of origin (which are stable across datasets). Dataset 2015 is not included in the next steps as the headers are not correctly imported."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b322bef",
   "metadata": {},
   "source": [
    "### Developers' background ###\n",
    "\n",
    "Now we can try to observe how the background of the participants has changed over the years. We will focus on sociodemographic factors such as country of origin and gender, which are the most common features in every dataset and should have been correctly uploaded. We can therefore see which country represent the most promising market for IT and whether IT is a gender biased world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aab4760",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Country of origin\n",
    "# we will create a dictionary storing the percentage of respondents from the different countries according to each df.\n",
    "\n",
    "country_perc = {} # initialize dataframe\n",
    "dfs = list(d.keys())\n",
    "dfs.remove('df_2015') # bad upload\n",
    "for df in dfs:\n",
    "    if df in ['df_2011','df_2012','df_2013']:\n",
    "        country_key = 'What Country or Region do you live in?'\n",
    "    elif df == 'df_2014':\n",
    "        country_key = 'What Country do you live in?'\n",
    "    elif df == 'df_2016':\n",
    "        country_key = 'country'\n",
    "    elif df in ['df_2017','df_2018','df_2019','df_2020','df_2021']:\n",
    "        country_key = 'Country'\n",
    "    country_perc[df] = d[df][country_key].value_counts()/d[df].shape[0]\n",
    "    \n",
    "country_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afd3584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the participants' percentages by country across the years:\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 2, ncols = 5, constrained_layout = True, figsize = (12.7,9.27)) # we make subplots\n",
    "c = 0 # counter to iteratively index the right dataframe\n",
    "for ax in axes.flatten():\n",
    "    # plot\n",
    "    ax = sns.barplot(x = country_perc[dfs[c]][:10].index, y = country_perc[dfs[c]][:10].values, ax = ax) # bar plot of the top 10 values\n",
    "    ax.set_xticklabels(ax.get_xticklabels(),rotation = 90) # rotate the x labels\n",
    "    ax.title.set_text(dfs[c][-4:]) # give a title\n",
    "    c = c + 1 # update the counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371b506b",
   "metadata": {},
   "source": [
    "It appears that USA has been the country where most responders come from, across the years. However, what is interesting here is the increasing contribution of India, which allowed it to move from 7th place to steadily 2nd from 2017 onwards. India could therefore represent a solid market, apart from US, to hire new employees or sell IT products. Let's now give a look at possible gender differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f408be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Gender of respondents\n",
    "\n",
    "gender_perc = {}\n",
    "dfs = list(d.keys())\n",
    "dfs = [e for e in dfs if e not in ('df_2011','df_2012','df_2013','df_2014','df_2015','df_2016')] # in these datasets gender was not recorded\n",
    "for df in dfs:\n",
    "    gender_key = 'Gender'\n",
    "    gender_perc[df] = d[df][gender_key].value_counts()/d[df].shape[0]\n",
    "gender_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a1cbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot gender or respondents across the last 4 years of the survey:\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 5, ncols = 1, constrained_layout = True, figsize = (12.7,9.27))\n",
    "c = 0\n",
    "for ax in axes.flatten():\n",
    "    # plot\n",
    "    patches, texts = ax.pie(gender_perc[dfs[c]][:3].values, normalize = True, shadow = True, startangle = 90) # pie chart\n",
    "    ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "    ax.legend(patches, gender_perc[dfs[c]][:3].index, loc = 'lower center')\n",
    "    ax.title.set_text(dfs[c][-4:])\n",
    "    c = c + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5872625e",
   "metadata": {},
   "source": [
    "We can clearly observe that, in proportion, males have been the majority of responders across the last few years (compared to females and others), which makes clear how a gender gap is still present amongst the developers and computer science world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaa5ed3",
   "metadata": {},
   "source": [
    "### Programming language trend across the years ###\n",
    "\n",
    "We will now see which one has been the main language used for working amongst developers that participated to the survey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc0899c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language of use, we need to change the key for different datasets as they were encoded differently\n",
    "\n",
    "language_perc = {}\n",
    "dfs = list(d.keys())\n",
    "dfs.remove('df_2015')\n",
    "for df in dfs:\n",
    "    language_key = 'Which languages are you proficient in?'\n",
    "    if df in ('df_2013','df_2014'):\n",
    "        language_key = 'Which of the following languages or technologies have you used significantly in the past year?'\n",
    "    elif df in ('df_2016'):\n",
    "        language_key = 'tech_do'\n",
    "    elif df in ('df_2017'):\n",
    "        language_key = 'HaveWorkedLanguage'\n",
    "    elif df in ('df_2018','df_2019','df_2020',''):\n",
    "        language_key = 'LanguageWorkedWith'\n",
    "    elif df == 'df_2021':\n",
    "        language_key = 'LanguageHaveWorkedWith'\n",
    "    language_perc[df] = d[df][language_key].value_counts()/d[df].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e30293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot the most used languages\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 2, ncols = 5, constrained_layout = True, figsize = (12.7,9.27))\n",
    "c = 0\n",
    "for ax in axes.flatten():\n",
    "    # plot\n",
    "    ax = sns.barplot(x = language_perc[dfs[c]][:10].index, y = language_perc[dfs[c]][:10].values, ax = ax)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(),rotation = 90)\n",
    "    ax.title.set_text(dfs[c][-4:])\n",
    "    c = c + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc66be9",
   "metadata": {},
   "source": [
    "As we can see we have a bit of confusion across the last datasets as participants were allowed to reply using multiple choices, we need to clean this to get a better idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91de95c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two functions that we need for cleaning the mess about the languages\n",
    "\n",
    "def total_count(df, col1, col2, look_for):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df - the pandas dataframe you want to search\n",
    "    col1 - the column name you want to look through\n",
    "    col2 - the column you want to count values from\n",
    "    look_for - a list of strings you want to search for in each row of df[col]\n",
    "\n",
    "    OUTPUT:\n",
    "    new_df - a dataframe of each look_for with the count of how often it shows up\n",
    "    '''\n",
    "    new_df = defaultdict(int)\n",
    "    #loop through list of ed types\n",
    "    for val in look_for:\n",
    "        #loop through rows\n",
    "        for idx in range(df.shape[0]):\n",
    "            #if the ed type is in the row add 1\n",
    "            if val in df[col1][idx]:\n",
    "                new_df[val] += int(df[col2][idx])\n",
    "    new_df = pd.DataFrame(pd.Series(new_df)).reset_index()\n",
    "    new_df.columns = [col1, col2]\n",
    "    new_df.sort_values('count', ascending = False, inplace = True)\n",
    "    return new_df\n",
    "\n",
    "def clean_and_plot(df,var_name, title = 'Programming language preferred', plot = True):\n",
    "    '''\n",
    "    INPUT \n",
    "        df - a dataframe holding the var_name column\n",
    "        title - string the title of your plot\n",
    "        axis - axis object\n",
    "        plot - bool providing whether or not you want a plot back\n",
    "        \n",
    "    OUTPUT\n",
    "        cln_df - a dataframe with the count of how many individuals\n",
    "        Displays a plot of pretty things related to the var_name column.\n",
    "    '''\n",
    "    cln = df[var_name].value_counts().reset_index()\n",
    "    cln.rename(columns={'index': 'Language', var_name: 'count'}, inplace = True)\n",
    "    cln_df = total_count(cln, 'Language', 'count', possible_vals)\n",
    "\n",
    "    cln_df.set_index('Language', inplace = True)\n",
    "    if plot:\n",
    "        (cln_df/cln_df.sum()).plot(kind='bar', legend=None);\n",
    "        plt.title(title);\n",
    "        plt.show()\n",
    "    props_cln_df = cln_df/cln_df.sum()\n",
    "    return props_cln_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57006a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "possible_vals = ['Java','Javascript','C#','C','SQL','SQL Server','Android',\n",
    "                 'PHP','C++','Python','HTML/CCS','Android','HTML',\n",
    "                'CSS','TypeScript','Bash/Shell'] # possible languages answer, needed for clean_and_plot\n",
    "\n",
    "# let's clean the data:\n",
    "\n",
    "language_cleaned = {}\n",
    "dfs = list(d.keys())\n",
    "dfs.remove('df_2015')\n",
    "\n",
    "for df in dfs:\n",
    "    language_key = 'Which languages are you proficient in?'\n",
    "    if df in ('df_2013','df_2014'):\n",
    "        language_key = 'Which of the following languages or technologies have you used significantly in the past year?'\n",
    "    elif df in ('df_2016'):\n",
    "        language_key = 'tech_do'\n",
    "    elif df in ('df_2017'):\n",
    "        language_key = 'HaveWorkedLanguage'\n",
    "    elif df in ('df_2018','df_2019','df_2020'):\n",
    "        language_key = 'LanguageWorkedWith'\n",
    "    elif df == 'df_2021':\n",
    "        language_key = 'LanguageHaveWorkedWith' \n",
    "    language_cleaned[df] = clean_and_plot(d[df],var_name = language_key, plot = False) # we do not need the plot, we will do later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc222f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now we plot the languages again, following cleaning\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 2, ncols = 5, constrained_layout = True, figsize = (12.7,9.27))\n",
    "c = 0\n",
    "for ax in axes.flatten():\n",
    "    # plot\n",
    "    ax = sns.barplot(x = language_cleaned[dfs[c]][:10].index, y = language_cleaned[dfs[c]]['count'][:10].values, ax = ax)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(),rotation = 90)\n",
    "    ax.title.set_text(dfs[c][-4:])\n",
    "    c = c + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65b6f40",
   "metadata": {},
   "source": [
    "What we can observe is that the majority developers use (or has used) C and Java. However, it is also clear that the developers' community has become increasingly diverse, thus allowing market for other languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fd2774",
   "metadata": {},
   "source": [
    "### Job opportunities for Java and C developers ###\n",
    "\n",
    "We will now try to see whether being a Java or C developer is able to successfully predict a high salary more than other languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef72322f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use only the 2017 survey\n",
    "\n",
    "df_model = pd.read_csv(main_dir + '/unzip_datasets/2017/survey_results_public.csv')\n",
    "\n",
    "# and we are interested in the salary and programming languages variable\n",
    "df_model = df_model[['Salary','HaveWorkedLanguage']]\n",
    "\n",
    "# let's clean the data, Salary is our target variable\n",
    "df_model_c = df_model.dropna(subset = ['Salary'])\n",
    "\n",
    "# let's get the highest portion of salaries (top 50%)\n",
    "df_model_c = df_model_c.sort_values(by = 'Salary',  ascending = False)\n",
    "df_model_c = df_model_c[:np.round(df_model_c.shape[0]/2).astype(int)]\n",
    "\n",
    "# we have a 0.1 percent of missing values in the language variable, we can use it without problem.\n",
    "# However, we need to dummy it\n",
    "df_model_c['HaveWorkedLanguage'].isnull().mean()\n",
    "\n",
    "# for this we will use a custom made function\n",
    "\n",
    "def create_dummy_df(df, cat_cols, dummy_na):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df - pandas dataframe with categorical variables you want to dummy\n",
    "    cat_cols - list of strings that are associated with names of the categorical columns\n",
    "    dummy_na - Bool holding whether you want to dummy NA vals of categorical columns or not\n",
    "    \n",
    "    OUTPUT:\n",
    "    df - a new dataframe that has the following characteristics:\n",
    "            1. contains all columns that were not specified as categorical\n",
    "            2. removes all the original columns in cat_cols\n",
    "            3. dummy columns for each of the categorical columns in cat_cols\n",
    "            4. if dummy_na is True - it also contains dummy columns for the NaN values\n",
    "            5. Use a prefix of the column name with an underscore (_) for separating \n",
    "    '''\n",
    "    for col in cat_cols:\n",
    "        try:\n",
    "            # for each cat add dummy var, drop original column\n",
    "            df = pd.concat([df.drop(col,axis = 1),pd.get_dummies(df[col],prefix = col, prefix_sep = '_', drop_first = True, dummy_na = dummy_na)],axis = 1)\n",
    "        except:\n",
    "            continue\n",
    "    return df\n",
    "\n",
    "df_model_c_final = create_dummy_df(df_model_c, cat_cols = ['HaveWorkedLanguage'], dummy_na = False) # our final dummy dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a205ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can create a model, we will use a linear regression\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score,mean_squared_error\n",
    "\n",
    "\n",
    "X = df_model_c_final.drop('Salary',axis = 1) # all the dummy variables as predictors \n",
    "y = df_model_c_final['Salary'] # predict (high) salary\n",
    "                      \n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = .3, random_state = 42) # randomly splitting the test, random_state is useful to have the very same split\n",
    "\n",
    "lm_model = LinearRegression(normalize = True)\n",
    "lm_model.fit(X_train,y_train) #fitting the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d5dbcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we can now predict salary and extract metrics of validity + the weight of the most important variables contributing to the model\n",
    "\n",
    "def coef_weights(coefficients, X_train):\n",
    "    '''\n",
    "    INPUT:\n",
    "    coefficients - the coefficients of the linear model \n",
    "    X_train - the training data, so the column names can be used\n",
    "    OUTPUT:\n",
    "    coefs_df - a dataframe holding the coefficient, estimate, and abs(estimate)\n",
    "    \n",
    "    Provides a dataframe that can be used to understand the most influential coefficients\n",
    "    in a linear model by providing the coefficient estimates along with the name of the \n",
    "    variable attached to the coefficient.\n",
    "    '''\n",
    "    coefs_df = pd.DataFrame()\n",
    "    coefs_df['est_int'] = X_train.columns\n",
    "    coefs_df['coefs'] = lm_model.coef_\n",
    "    coefs_df['abs_coefs'] = np.abs(lm_model.coef_)\n",
    "    coefs_df = coefs_df.sort_values('abs_coefs', ascending=False)\n",
    "    return coefs_df\n",
    "\n",
    "\n",
    "y_preds = lm_model.predict(X_test) # predict using the test set\n",
    "mse = mean_squared_error(y_preds,y_test) # calculate mse\n",
    "r2 = r2_score(y_preds,y_test) # calculate r2\n",
    "coef_df = coef_weights(lm_model.coef_, X_train) # extract parameters coefficients\n",
    "\n",
    "print('our mse is: %.2f' %mse)\n",
    "print('our r2 is: %.5f' %r2)\n",
    "coef_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59dc82c",
   "metadata": {},
   "source": [
    "Java and C are indeed present amongst the combinations which influences our predictions the most. However, the R2 and MSE metrics are so poor that we defintively rule out our model as valid. Therefore, high salary cannot be predict only as a combination of languages learned/used (how suprising!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
